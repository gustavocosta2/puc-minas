*aula dia 18/09/2024*

# **Pré-processamento de Dados**


O pré-processamento de dados é uma etapa essencial na análise e ciência de dados, pois garante que as informações estejam em uma forma limpa e organizada para os modelos de machine learning. 
As principais etapas incluem:

- **Limpeza de Dados:** Remoção de dados ausentes, duplicados e incorretos. Isso pode envolver a imputação de valores para preencher lacunas, dependendo da necessidade do projeto.

- **Transformação de Dados:** Inclui normalização ou padronização de variáveis numéricas, de forma que diferentes escalas não afetem o desempenho do modelo. Dados categóricos também são
transformados (e.g., one-hot encoding).

- **Redução de Dimensionalidade:** Utilizada para simplificar o conjunto de dados e reduzir o tempo de processamento, usando técnicas como PCA (Principal Component Analysis).

- **Divisão do Conjunto de Dados:** Separação dos dados em conjuntos de treino, validação e teste, permitindo avaliar o desempenho e evitar overfitting.

Esse processo assegura que os dados estejam em um formato adequado, melhorando a precisão e a eficiência dos modelos.

## **PARTE 1 - Balanceamento**
Quando as classes são desbalanceadas, o modelo aprenderá com mais facilidade aquela classe majoritária no conjunto de treinamento. Para contornar essa situação temos algumas maneiras de se
resolver:

- 1° Forma: Retirar instâncias da classe majoritária (UNDERSAMPLING)

  - Retirar instâncias de forma aleatória.
  - Utilizar métodos heurísticos para selecionar as instâncias mais representativas.
  - métodos underSampler dependendo da quantidade de dados que serão removidos, eles não podem ser APAGADOS. Os que forem removidos do conjunto de TREINO podem ser inseridos no conjunto
  de TESTE. 

## **PARTE 2 - Dados Ausentes**
Nem sempre é viável apenas descartar os dados ausentes porque eles podem acarretar em *perda de informação* e também impacta *a quantidade de dados totais*. Por isso, é necessário sempre
procurar formas de resolver essa situação:

- Tentar preencher as ausências, na maioria dos casos não é possível apenas preenchê-los porque não é possível ir atrás dessas informações.
- Imputar dados ausentes com técnicas tradicionais, como a média e mediana para dados numéricos e moda para dados categóricos, mas não é recomendado porque gera muita *inconsistência.*
- Imputar valores fixos, é ruim porque você está colocando um valor que nem sempre é representativo.
- usando técnicas de Machine Learning como KNN Imputer que interpola valores artificiais, MISS Forest, etc.

## **PARTE 3 - Dados Inconsistentes e Redundantes**
Dados inconsistentes são jogados fora, não há como escolher uma instância certa. Já a redundância são instâncias idênticas, isso deixa o modelo *viciado*, portanto, as repetições precisam
ser eliminadas.

## **PARTE 4 - Conversão Simbólica-Numérica**

- Dados que sejam dicotômicos ou ordinais, é sugerido o uso do LabelEncoder.
- Não Ordinal com *poucas opções*: utilização do OneHotEncoder.
- Não Ordinal com *muitas opções*: criar novas codificações que representem a informação desses dados.

## **PARTE 5 - Redução de Dimensionalidade**
Esse passo consiste em reduzir a dimensionalidade, isso porque os modelos não têm uma boa performance com uma alta dimensionalidade. Esse passo necessita de *cuidado com a perda de informação*.
  
